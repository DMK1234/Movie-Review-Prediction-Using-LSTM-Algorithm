import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, BatchNormalization
from sklearn.model_selection import train_test_split
import tensorflow as tf
import matplotlib.pyplot as plt

# Load dataset
movie_reviews = pd.read_csv(r"C:\Users\Navya Vanka\Downloads\IMDB Dataset.csv")

# Check for null values
print(movie_reviews.isnull().values.any())

# Shape of the dataset
print(movie_reviews.shape)

# Preview the first few rows
print(movie_reviews.head())

# Preprocessing function to remove HTML tags and unwanted characters
TAG_RE = re.compile(r'<[^>]+>')

def remove_tags(text):
    return TAG_RE.sub('', text)

def preprocess_text(sentence):
    # Remove HTML tags
    sentence = remove_tags(sentence)

    # Remove non-alphabetic characters and make lowercase
    sentence = re.sub('[^a-zA-Z]', ' ', sentence).lower()

    # Remove single characters
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Remove extra spaces
    sentence = re.sub(r'\s+', ' ', sentence).strip()

    return sentence

# Apply preprocessing on the reviews using pandas' apply
movie_reviews['processed_review'] = movie_reviews['review'].apply(preprocess_text)

# Convert sentiment to binary (1 for positive, 0 for negative)
y = np.array([1 if sentiment == 'positive' else 0 for sentiment in movie_reviews['sentiment']])

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(movie_reviews['processed_review'], y, test_size=0.2, random_state=42)

# Tokenization
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Padding sequences to ensure uniform input length
maxlen = 256
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

# Load GloVe embeddings
embeddings_dictionary = dict()
with open(r'C:\Users\Navya Vanka\Downloads\glove.6B.100d.txt', encoding='ISO-8859-1') as glove_file:
    for line in glove_file:
        # Skip empty lines or lines with unexpected structure
        if not line.strip():
            continue
        
        # Split the line into individual components
        records = line.split()
        
        # Check if the line has enough parts (at least 2, one for word and one for vector)
        if len(records) < 2:
            continue
        
        word = records[0]
        try:
            # Convert the rest of the line to a float array
            vector_dimensions = np.asarray(records[1:], dtype='float32')
            embeddings_dictionary[word] = vector_dimensions
        except ValueError:
            # If conversion fails, skip this word (maybe due to unexpected characters)
            continue

# Create the embedding matrix
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 100  # Update embedding dimension to match GloVe
embedding_matrix = np.zeros((vocab_size, embedding_dim))

for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

# Build Improved LSTM Model
model = Sequential()

# Embedding Layer
embedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=True)
model.add(embedding_layer)

# LSTM Layers with Dropout
model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.3))
model.add(BatchNormalization())

model.add(LSTM(64, return_sequences=False))
model.add(Dropout(0.3))

# Fully Connected Layer
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))

# Output Layer
model.add(Dense(1, activation='sigmoid'))

# Compile the Model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Print model summary
model.summary()

# Train the Model
history = model.fit(X_train, y_train, batch_size=64, epochs=10, validation_split=0.2, verbose=1)

# Evaluate the Model
score = model.evaluate(X_test, y_test, verbose=1)
print("Test Loss:", score[0])
print("Test Accuracy:", score[1])

# Plot Accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot Loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
